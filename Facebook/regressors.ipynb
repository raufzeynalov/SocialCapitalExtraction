{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from catboost import CatBoostRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from models.hyperopt_model import HyperoptModel\n",
    "from models.utils import CustomDataFrameMapper, get_features, get_svr_features\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr,  spearmanr,  kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "preprocessed_data = pd.read_pickle('./data/preprocesed_data.pd')\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create two new dataframes, one with the training rows, one with the test rows\n",
    "train, test=train_test_split(df, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in the training data: 9303\n",
      "Number of observations in the test data: 2326\n"
     ]
    }
   ],
   "source": [
    "# Show the number of observations for the test and training dataframes\n",
    "print('Number of observations in the training data:', len(train))\n",
    "print('Number of observations in the test data:',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats (train | test):\n",
      "\tR^2 score:\t\t0.8565\n",
      "\t\t\t\t\t0.8021\n",
      "\tRMSE:\t\t\t0.0543\n",
      "\t\t\t\t\t0.0639\n",
      "\tMean error:\t\t0.0413\n",
      "\t\t\t\t\t0.0489\n",
      "\tPearson:\t\t0.9260\n",
      "\t\t\t\t\t0.8956\n",
      "\tSpearman:\t\t0.9140\n",
      "\t\t\t\t\t0.8829\n",
      "\tKendallTau:\t\t0.7526\n",
      "\t\t\t\t\t0.7082\n",
      "\n",
      "Plotting predicted vs. actual ...done\n",
      "\n",
      "[14.627768243851087, 0.26611010965975174, 0.2280542307257879, 0.14958020618385887, 0.1810367164379663, 0.35266106966566646, 0.393718394698887, 1.3765718623294974, 23.610246490702558, 11.847929726658172, 2.2174420422462084, 2.138930239388131, 4.42185213172175, 2.5831550972605215, 1.9170008067008786, 6.3582630385605, 26.31305854367411, 1.0166210495346484]\n",
      "Plotting feature importances ...done\n",
      "\n",
      "Plotting QQ ...done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    # Run CatBoostRegressor with hyperopt optimization\n",
    "    DO_LOWESS = False\n",
    "    hyperopt = False\n",
    "    # Run CatBoostRegressor with hyperopt optimization\n",
    "    cat = HyperoptModel(train.copy(), test.copy(),'cat', cv=3)\n",
    "    cat.raw_features = []\n",
    "    comment_features, mention_features, post_features, user_features = get_features()\n",
    "\n",
    "    cat.pipeline = Pipeline([\n",
    "        ('prepare_features', FeatureUnion([\n",
    "            ('user_stats', user_features),\n",
    "            ('mention_stats', mention_features),\n",
    "            ('posts_stats', post_features),\n",
    "            ('comments_stats', comment_features)\n",
    "        ])),\n",
    "        ('estimate', CatBoostRegressor(**{'bagging_temperature': 0.7581941022055045,\n",
    "                                          'iterations': 1300,\n",
    "                                          'l2_leaf_reg': 3.3369699206218777,\n",
    "                                          'learning_rate': 0.03533373267651768,\n",
    "                                          'loss_function': 'RMSE',\n",
    "                                          'random_seed': 0,\n",
    "                                          'random_strength': 1,\n",
    "                                          'thread_count': 4,\n",
    "                                          'train_dir': 'outputs/cat'}))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    for transformer in cat.pipeline.named_steps['prepare_features'].transformer_list:\n",
    "        cat.raw_features += [t[0] if isinstance(t[0], str) else t[0][0] for t in transformer[1].features]\n",
    "   \n",
    "    cat.space = {\n",
    "        'estimate__iterations': hp.choice('estimate__iterations', [1300]),\n",
    "        'estimate__loss_function': hp.choice('estimate__loss_function', ['RMSE']),\n",
    "        'estimate__train_dir': hp.choice('estimate__train_dir', ['outputs/cat']),\n",
    "        'estimate__thread_count': hp.choice('estimate__thread_count', [4]),\n",
    "        'estimate__used_ram_limit': hp.choice('estimate__used_ram_limit', [1024 * 1024 * 1024 * 4]),  # 4gb\n",
    "        'estimate__random_seed': hp.choice('estimate__random_seed', [0]),\n",
    "\n",
    "        'estimate__learning_rate': hp.loguniform('estimate__learning_rate', -5, 0),\n",
    "        'estimate__random_strength': hp.choice('estimate__random_strength', [1, 20]),\n",
    "        'estimate__l2_leaf_reg': hp.loguniform('estimate__l2_leaf_reg', 0, np.log(10)),\n",
    "        'estimate__bagging_temperature': hp.uniform('estimate__bagging_temperature', 0, 1),\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    find the best number of trees (following https://tech.yandex.com/catboost/doc/dg/concepts/parameter-tuning_trees-number-docpage/#parameter-tuning_trees-number)\n",
    "    need to also substitute the CatBoostRegressor parameters with:\n",
    "        ('estimate', CatBoostRegressor(iterations=10000, loss_function='RMSE', auto_stop_pval=1e-4, use_best_model=True, train_dir='outputs/cat_trees', verbose=True))\n",
    "    \"\"\"\n",
    "    # num_trees_train, num_trees_eval = train_test_split(train, test_size=0.2, random_state=0)\n",
    "    # X = num_trees_train[list(filter(lambda column: column in cat.raw_features, cat.train.columns))]\n",
    "    # y = num_trees_train['score']\n",
    "    #\n",
    "    # eval_X = num_trees_eval[list(filter(lambda column: column in cat.raw_features, cat.train.columns))]\n",
    "    # eval_X = cat.pipeline.named_steps['prepare_features'].fit_transform(eval_X)\n",
    "    # eval_set = (eval_X, num_trees_eval['score'])\n",
    "    #\n",
    "    # cat.pipeline.fit(X=X, y=y, estimate__eval_set=eval_set)\n",
    "    # cat.model = cat.pipeline\n",
    "    # print(cat.model.named_steps['estimate'].get_params())\n",
    "    # cat.model.named_steps['estimate'].save_model('tmp/cat.model'.encode('utf-8'))\n",
    "\n",
    "    if hyperopt:\n",
    "        cat.run(do_lowess=DO_LOWESS)\n",
    "    else:\n",
    "        # train with default params\n",
    "        cat.pipeline.fit(X=cat.X_train, y=cat.y_train)\n",
    "        cat.model = cat.pipeline\n",
    "        cat.stats()\n",
    "        cat.plot_predicted_vs_actual(do_lowess=DO_LOWESS)\n",
    "        cat.plot_feature_importance()\n",
    "        cat.qq_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats (train | test):\n",
      "\tR^2 score:\t\t0.8830\n",
      "\t\t\t\t\t0.7929\n",
      "\tRMSE:\t\t\t0.0490\n",
      "\t\t\t\t\t0.0653\n",
      "\tMean error:\t\t0.0376\n",
      "\t\t\t\t\t0.0501\n",
      "\tPearson:\t\t0.9405\n",
      "\t\t\t\t\t0.8905\n",
      "\tSpearman:\t\t0.9309\n",
      "\t\t\t\t\t0.8779\n",
      "\tKendallTau:\t\t0.7778\n",
      "\t\t\t\t\t0.7014\n",
      "\n",
      "Plotting predicted vs. actual ...done\n",
      "\n",
      "Plotting feature importances ...done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    # Run LGBMRegressor with hyperopt optimization\n",
    "    DO_LOWESS = False\n",
    "    hyperopt = False\n",
    "    lgbm = HyperoptModel(train.copy(), test.copy(), 'lgbm', cv=5)\n",
    "    comment_features, mention_features, post_features, user_features = get_features()\n",
    "    lgbm.raw_features = []\n",
    "    lgbm.pipeline = Pipeline([\n",
    "        ('prepare_features', FeatureUnion([\n",
    "            ('user_stats', user_features),\n",
    "            ('mention_stats', mention_features),\n",
    "            ('posts_stats', post_features),\n",
    "            ('comments_stats', comment_features)\n",
    "        ])),\n",
    "        ('estimate', lgb.LGBMRegressor(**{'bagging_fraction': 0.7394781981136033,\n",
    "                                          'feature_fraction': 0.9385660204777418,\n",
    "                                          'lambda_l1': 0.020835973532181865,\n",
    "                                          'lambda_l2': 0.03941529177232025,\n",
    "                                          'learning_rate': 0.035919305210976936,\n",
    "                                          'min_data_in_leaf': 17,\n",
    "                                          'min_sum_hessian_in_leaf': 1.3190919274009395e-05,\n",
    "                                          'n_estimators': 400,\n",
    "                                          'num_leaves': 33,\n",
    "                                          'objective': 'regression',\n",
    "                                          'seed': 0}))\n",
    "    ])\n",
    "    for transformer in lgbm.pipeline.named_steps['prepare_features'].transformer_list:\n",
    "        lgbm.raw_features += [t[0] if isinstance(t[0], str) else t[0][0] for t in transformer[1].features]\n",
    "   \n",
    "    \"\"\" find number of trees \"\"\"\n",
    "    # num_trees_train, num_trees_eval = train_test_split(train, test_size=0.2, random_state=0)\n",
    "    # X = num_trees_train[list(filter(lambda column: column in lgbm.raw_features, lgbm.train.columns))]\n",
    "    # y = num_trees_train['score']\n",
    "    #\n",
    "    # eval_X = num_trees_eval[list(filter(lambda column: column in lgbm.raw_features, lgbm.train.columns))]\n",
    "    # eval_X = lgbm.pipeline.named_steps['prepare_features'].fit_transform(eval_X)\n",
    "    # eval_set = (eval_X, num_trees_eval['score'])\n",
    "    #\n",
    "    # best = lgbm.pipeline.fit(X=X, y=y, estimate__eval_set=eval_set, estimate__early_stopping_rounds=10)\n",
    "    # print(best.named_steps['estimate'].best_iteration)\n",
    "\n",
    "    lgbm.space = {\n",
    "        'estimate__objective': hp.choice('estimate__objective', ['regression']),\n",
    "        'estimate__n_estimators': hp.choice('estimate__n_estimators', [400]),\n",
    "        'estimate__seed': hp.choice('estimate__seed', [0]),\n",
    "\n",
    "        'estimate__learning_rate': hp.loguniform('estimate__learning_rate', -7, 0),\n",
    "        'estimate__num_leaves': scope.int(hp.qloguniform('estimate__num_leaves', 1, 7, 1)),\n",
    "        'estimate__feature_fraction': hp.uniform('estimate__feature_fraction', 0.5, 1),\n",
    "        'estimate__bagging_fraction': hp.uniform('estimate__bagging_fraction', 0.5, 1),\n",
    "        'estimate__min_data_in_leaf': scope.int(hp.qloguniform('estimate__min_data_in_leaf', 0, 6, 1)),\n",
    "        'estimate__min_sum_hessian_in_leaf': hp.loguniform('estimate__min_sum_hessian_in_leaf', -16, 5),\n",
    "        'estimate__lambda_l1': hp.choice('lambda_l1', [0, hp.loguniform('estimate__lambda_l1_positive', -16, 2)]),\n",
    "        'estimate__lambda_l2': hp.choice('lambda_l2', [0, hp.loguniform('estimate__lambda_l2_positive', -16, 2)]),\n",
    "    }\n",
    "\n",
    "    if hyperopt:\n",
    "        lgbm.run(do_lowess=DO_LOWESS)\n",
    "    else:\n",
    "        # train with default params\n",
    "        lgbm.pipeline.fit(X=lgbm.X_train, y=lgbm.y_train)\n",
    "        lgbm.model = lgbm.pipeline\n",
    "        lgbm.stats()\n",
    "        lgbm.plot_predicted_vs_actual()\n",
    "        lgbm.plot_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats (train | test):\n",
      "\tR^2 score:\t\t0.9651\n",
      "\t\t\t\t\t0.7557\n",
      "\tRMSE:\t\t\t0.0268\n",
      "\t\t\t\t\t0.0710\n",
      "\tMean error:\t\t0.0203\n",
      "\t\t\t\t\t0.0544\n",
      "\tPearson:\t\t0.9857\n",
      "\t\t\t\t\t0.8708\n",
      "\tSpearman:\t\t0.9845\n",
      "\t\t\t\t\t0.8594\n",
      "\tKendallTau:\t\t0.8960\n",
      "\t\t\t\t\t0.6770\n",
      "\n",
      "Plotting predicted vs. actual ...done\n",
      "\n",
      "Plotting feature importances ...done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    #parameters\n",
    "    DO_LOWESS = False\n",
    "    hyperopt = False\n",
    "    # Run RandomForestRegressor with hyperopt optimization\n",
    "    rf = HyperoptModel(train.copy(), test.copy(), 'rf', cv=3)\n",
    "    rf.raw_features = []\n",
    "    comment_features, mention_features, post_features, user_features = get_features()\n",
    "    rf.pipeline = Pipeline([\n",
    "        ('prepare_features', FeatureUnion([\n",
    "            ('user_stats', user_features),\n",
    "            ('mention_stats', mention_features),\n",
    "            ('posts_stats', post_features),\n",
    "            ('comments_stats', comment_features)\n",
    "        ])),\n",
    "        ('estimate', RandomForestRegressor(**{'max_features': 0.5954111977396234,\n",
    "                                              'min_samples_leaf': 1,\n",
    "                                              'n_estimators': 388,\n",
    "                                              'oob_score': True,\n",
    "                                              'random_state': 0}))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    for transformer in rf.pipeline.named_steps['prepare_features'].transformer_list:\n",
    "        rf.raw_features += [t[0] if isinstance(t[0], str) else t[0][0] for t in transformer[1].features]\n",
    "     # for transformer in rf.pipeline.named_steps['prepare_features'].transformer_list:\n",
    "      #    rf.raw_features += [t if isinstance(t, basestring) else t[0] for t, _, _ in transformer[1].features]\n",
    "    #print(rf.get_params().keys())\n",
    "    rf.space = {\n",
    "        'estimate__random_state': hp.choice('estimate__random_state', [0]),\n",
    "        'estimate__oob_score': hp.choice('estimate__oob_score', [True]),\n",
    "\n",
    "        'estimate__max_features': hp.uniform('estimate__max_features', 0, 1.),\n",
    "        'estimate__n_estimators': hp.choice('estimate__n_estimators', range(1, 3000 + 1)),\n",
    "        'estimate__min_samples_leaf': hp.choice('estimate__min_samples_leaf', range(1, 100 + 1)),\n",
    "        #'estimate__scale': hp.choice('estimate__scale', [0, 1.]),\n",
    "        #'estimate__normalize': hp.choice('estimate__normalize', [0, 1.]),\n",
    "    }\n",
    "\n",
    "    if hyperopt:\n",
    "        rf.run(do_lowess=DO_LOWESS)\n",
    "    else:\n",
    "        # train with default params\n",
    "        rf.pipeline.fit(X=rf.X_train, y=rf.y_train)\n",
    "        rf.model = rf.pipeline\n",
    "        rf.stats()\n",
    "        rf.plot_predicted_vs_actual()\n",
    "        rf.plot_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats (train | test):\n",
      "\tR^2 score:\t\t0.6136\n",
      "\t\t\t\t\t0.5432\n",
      "\tRMSE:\t\t\t0.0891\n",
      "\t\t\t\t\t0.0970\n",
      "\tMean error:\t\t0.0682\n",
      "\t\t\t\t\t0.0741\n",
      "\tPearson:\t\t0.7835\n",
      "\t\t\t\t\t0.7371\n",
      "\tSpearman:\t\t0.7619\n",
      "\t\t\t\t\t0.7168\n",
      "\tKendallTau:\t\t0.5804\n",
      "\t\t\t\t\t0.5316\n",
      "\n",
      "Plotting predicted vs. actual ...done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    # Run SVM with hyperopt optimization\n",
    "    DO_LOWESS = False\n",
    "    hyperopt = False\n",
    "    \n",
    "    comment_features, mention_features, post_features, user_features = get_svr_features()\n",
    "    svm = HyperoptModel(train.copy(), test.copy(), 'svr', cv=3)\n",
    "    svm.pipeline = Pipeline([\n",
    "        ('prepare_features', FeatureUnion([\n",
    "            ('user_stats', user_features),\n",
    "            ('mention_stats', mention_features),\n",
    "            ('posts_stats', post_features),\n",
    "            # ('comments_stats', comment_features)\n",
    "        ])),\n",
    "        ('estimate', SVR(**{'C': 0.48823921084665933,\n",
    "                            'epsilon': 0.06249804864583549,\n",
    "                            'gamma': 0.1606868912867299,\n",
    "                            'kernel': 'rbf'}))\n",
    "    ])\n",
    "    \n",
    "    svm.raw_features = []\n",
    "    for transformer in svm.pipeline.named_steps['prepare_features'].transformer_list:\n",
    "        svm.raw_features += [t[0] if isinstance(t[0], str) else t[0][0] for t in transformer[1].features]\n",
    "   \n",
    "    svm.space = {\n",
    "        'estimate__C': hp.uniform('estimate__C', 0, 10.),\n",
    "        'estimate__kernel': hp.choice('estimate__kernel', ['linear', 'sigmoid', 'rbf']),\n",
    "        'estimate__gamma': hp.uniform('estimate__gamma', 0, 10.),\n",
    "        #'estimate__scale': hp.choice('estimate__scale', [0, 1.]),\n",
    "        #'estimate__normalize': hp.choice('estimate__normalize', [0, 1.]),\n",
    "    }\n",
    "    \n",
    "    if hyperopt:\n",
    "        svm.run(do_lowess=DO_LOWESS)\n",
    "    else:\n",
    "        # train with default params\n",
    "        svm.pipeline.fit(X=svm.X_train, y=svm.y_train)\n",
    "        svm.model = svm.pipeline\n",
    "        svm.stats()\n",
    "        svm.plot_predicted_vs_actual()\n",
    "        svm.plot_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats (train | test):\n",
      "\tR^2 score:\t\t0.6667\n",
      "\t\t\t\t\t0.6471\n",
      "\tRMSE:\t\t\t0.0828\n",
      "\t\t\t\t\t0.0853\n",
      "\tMean error:\t\t0.0653\n",
      "\t\t\t\t\t0.0670\n",
      "\tPearson:\t\t0.8169\n",
      "\t\t\t\t\t0.8054\n",
      "\tSpearman:\t\t0.7894\n",
      "\t\t\t\t\t0.7766\n",
      "\tKendallTau:\t\t0.6005\n",
      "\t\t\t\t\t0.5891\n",
      "\n",
      "Plotting predicted vs. actual ...done\n",
      "\n",
      "Plotting QQ ...done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    #some parameters\n",
    "    DO_LOWESS = False\n",
    "    hyperopt = False\n",
    "    # Run MLPRegressor with hyperopt optimization\n",
    "    comment_features, mention_features, post_features, user_features = get_features()\n",
    "    nn = HyperoptModel(train.copy(), test.copy(),'nn', cv=3, max_evals=2)\n",
    "    nn.raw_features = []\n",
    "\n",
    "    nn.pipeline = Pipeline([\n",
    "            ('prepare_features', FeatureUnion([\n",
    "            ('user_stats', user_features),\n",
    "            ('mention_stats', mention_features),\n",
    "            ('posts_stats', post_features),\n",
    "            ('comments_stats', comment_features)\n",
    "        ])),\n",
    "        ('estimate', MLPRegressor(**{'alpha': 0.00189292968732814,\n",
    "                                     'activation': 'logistic',\n",
    "                                     'hidden_layer_sizes': (88,),\n",
    "                                     'solver': 'adam'    }))\n",
    "    ])\n",
    "    \n",
    "    for transformer in nn.pipeline.named_steps['prepare_features'].transformer_list:\n",
    "        nn.raw_features += [t[0] if isinstance(t[0], str) else t[0][0] for t in transformer[1].features]\n",
    "   \n",
    "\n",
    "    nn.space = {\n",
    "         'estimate__alpha' : hp.uniform('estimate__alpha', 0.00001, 1),\n",
    "         'estimate__activation' : hp.choice('estimate__activation', ['logistic']), # 'identity', 'logistic', 'tanh', 'relu'\n",
    "         #'estimate__learning_rate' : hp.choice('estimate__learning_rate', ['constant', 'invscaling', 'adaptive']),\n",
    "         'estimate__hidden_layer_sizes' : (scope.int(hp.uniform('estimate__first_layer', 1, 100)),scope.int(hp.uniform('estimate__second_layer', 1, 100))) ,\n",
    "         'estimate__solver' : hp.choice('estimate__solver', ['adam']), #'lbfgs', 'sgd',\n",
    "         #'estimate__max_iter' : scope.int(hp.uniform('estimate__max_iter', 500, 1000))\n",
    "    }\n",
    "\n",
    "    if hyperopt:\n",
    "        nn.run(do_lowess=DO_LOWESS)\n",
    "    else:\n",
    "        # train with default params\n",
    "        nn.pipeline.fit(X=nn.X_train, y=nn.y_train)\n",
    "        nn.model = nn.pipeline\n",
    "        nn.stats()\n",
    "        nn.plot_predicted_vs_actual(do_lowess=DO_LOWESS)\n",
    "        nn.plot_feature_importance()\n",
    "        nn.qq_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 1\n",
      "Polynomial degree: 1\n",
      "Stats (train | test):\n",
      "\tR^2 score:\t\t0.5113\n",
      "\t\t\t\t\t0.5536\n",
      "\tRMSE:\t\t\t0.1008\n",
      "\t\t\t\t\t0.0939\n",
      "\tMean error:\t\t0.0724\n",
      "\t\t\t\t\t0.0701\n",
      "\tPearson:\t\t0.7150\n",
      "\t\t\t\t\t0.7441\n",
      "\tSpearman:\t\t0.8123\n",
      "\t\t\t\t\t0.8179\n",
      "\tKendallTau:\t\t0.6269\n",
      "\t\t\t\t\t0.6307\n",
      "Degree: 2\n",
      "Polynomial degree: 2\n",
      "Stats (train | test):\n",
      "\tR^2 score:\t\t0.7627\n",
      "\t\t\t\t\t0.6012\n",
      "\tRMSE:\t\t\t0.0696\n",
      "\t\t\t\t\t0.0920\n",
      "\tMean error:\t\t0.0484\n",
      "\t\t\t\t\t0.0532\n",
      "\tPearson:\t\t0.8734\n",
      "\t\t\t\t\t0.7863\n",
      "\tSpearman:\t\t0.9141\n",
      "\t\t\t\t\t0.8977\n",
      "\tKendallTau:\t\t0.7529\n",
      "\t\t\t\t\t0.7367\n",
      "Degree: 3\n",
      "Polynomial degree: 3\n",
      "Stats (train | test):\n",
      "\tR^2 score:\t\t0.8853\n",
      "\t\t\t\t\t-118.4447\n",
      "\tRMSE:\t\t\t0.0488\n",
      "\t\t\t\t\t1.5300\n",
      "\tMean error:\t\t0.0331\n",
      "\t\t\t\t\t0.1629\n",
      "\tPearson:\t\t0.9410\n",
      "\t\t\t\t\t0.1134\n",
      "\tSpearman:\t\t0.9583\n",
      "\t\t\t\t\t0.8091\n",
      "\tKendallTau:\t\t0.8338\n",
      "\t\t\t\t\t0.6715\n",
      "Degree: 4\n",
      "Polynomial degree: 4\n",
      "Stats (train | test):\n",
      "\tR^2 score:\t\t0.9699\n",
      "\t\t\t\t\t-362283208.4452\n",
      "\tRMSE:\t\t\t0.0249\n",
      "\t\t\t\t\t2717.0577\n",
      "\tMean error:\t\t0.0137\n",
      "\t\t\t\t\t104.7707\n",
      "\tPearson:\t\t0.9849\n",
      "\t\t\t\t\t0.0190\n",
      "\tSpearman:\t\t0.9895\n",
      "\t\t\t\t\t0.3737\n",
      "\tKendallTau:\t\t0.9247\n",
      "\t\t\t\t\t0.2924\n"
     ]
    }
   ],
   "source": [
    "    #Polynomial linear regression \n",
    "    degrees = 4\n",
    "    for d in range(1, degrees+1):\n",
    "            print(\"Degree: %s\" % d)\n",
    "            # Create the model, split the sets and fit it\n",
    "            polynomial_features = PolynomialFeatures(\n",
    "                degree=d, include_bias=False\n",
    "            )\n",
    "            linear_regression = linear_model.LinearRegression()\n",
    "            model = Pipeline([\n",
    "                (\"polynomial_features\", polynomial_features),\n",
    "                (\"linear_regression\", linear_regression)\n",
    "            ])\n",
    "            dfPoly = df.copy()\n",
    "            y = dfPoly['score'].copy()\n",
    "            dfPoly.drop('score', axis=1, inplace=True) \n",
    "\n",
    "            # create training and testing vars\n",
    "            X_train, X_test, y_train, y_test = train_test_split(dfPoly, y, test_size=0.2)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            # Calculate the metrics\n",
    "            train_prediction = model.predict(X_train)\n",
    "            test_prediction = model.predict(X_test)\n",
    "            \n",
    "            print('Polynomial degree: {}'.format(d))\n",
    "            print('Stats (train | test):')\n",
    "            print('\\tR^2 score:\\t\\t%.4f\\n\\t\\t\\t\\t\\t%.4f' % (r2_score(y_train, train_prediction),\n",
    "                                                        r2_score(y_test, test_prediction)))\n",
    "            print('\\tRMSE:\\t\\t\\t%.4f\\n\\t\\t\\t\\t\\t%.4f' % (mean_squared_error(y_train, train_prediction) ** 0.5,\n",
    "                                                     mean_squared_error(y_test, test_prediction) ** 0.5))\n",
    "            print('\\tMean error:\\t\\t%.4f\\n\\t\\t\\t\\t\\t%.4f' % (mean_absolute_error(y_train, train_prediction),\n",
    "                                                         mean_absolute_error(y_test, test_prediction)))\n",
    "            print('\\tPearson:\\t\\t%.4f\\n\\t\\t\\t\\t\\t%.4f' % (pearsonr(y_train, train_prediction)[0],\n",
    "                                                         pearsonr(y_test, test_prediction)[0]))\n",
    "            print('\\tSpearman:\\t\\t%.4f\\n\\t\\t\\t\\t\\t%.4f' % (spearmanr(y_train, train_prediction)[0],\n",
    "                                                         spearmanr(y_test, test_prediction)[0]))\n",
    "            print('\\tKendallTau:\\t\\t%.4f\\n\\t\\t\\t\\t\\t%.4f' % (kendalltau(y_train, train_prediction)[0],\n",
    "                                                         kendalltau(y_test, test_prediction)[0]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_friends_num', 'user_likes_per_post', 'user_friends_per_post',\n",
      "       'user_media_to_all_normal_ratio', 'user_normal_posts_num',\n",
      "       'user_life_events_num', 'user_small_posts_num',\n",
      "       'avg_normal_post_length', 'user_comments_num', 'avg_comment_length',\n",
      "       'user_likes_per_comment', 'comments_on_own_posts_num',\n",
      "       'comments_on_own_life_events_num', 'highest_education_level',\n",
      "       'education_is_present', 'languages_num', 'language_info_is_present',\n",
      "       'unique_mention_authors', 'mentions_num', 'own_mentions_num',\n",
      "       'unique_mention_authors_per_friend', 'mentions_per_friend',\n",
      "       'own_mentions_per_friend'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
