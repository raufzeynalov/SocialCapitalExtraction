{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from models.utils import CustomDataFrameMapper\n",
    "from models.utils import get_features_twitter_qualitative\n",
    "\n",
    "from models.hyperopt_model import HyperoptModel\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr,  spearmanr,  kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "preprocessed_data = pd.read_pickle('../data/preprocessed_twitter_qualitative.pd')\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame(preprocessed_data)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create two new dataframes, one with the training rows, one with the test rows\n",
    "train, test=train_test_split(df, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in the training data: 18680\n",
      "Number of observations in the test data: 4670\n"
     ]
    }
   ],
   "source": [
    "# Show the number of observations for the test and training dataframes\n",
    "print('Number of observations in the training data:', len(train))\n",
    "print('Number of observations in the test data:',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_features, retweet_features, tweet_features, user_features = get_features_twitter_qualitative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #some parameters\n",
    "    DO_LOWESS = False\n",
    "    hyperopt = True\n",
    "    # Run Linear Regression with hyperopt optimization\n",
    "    linear = HyperoptModel(train.copy(), test.copy(),'linear', cv=4, max_evals = 500)\n",
    "    linear.raw_features = []\n",
    "    linear.pipeline = Pipeline([\n",
    "          ('prepare_features', FeatureUnion([\n",
    "            ('user_features', user_features),\n",
    "            ('tweet_features', tweet_features),\n",
    "            ('retweet_features', retweet_features),\n",
    "            ('reply_features', reply_features)\n",
    "        ])),\n",
    "        ('estimate', linear_model.LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    for transformer in linear.pipeline.named_steps['prepare_features'].transformer_list:\n",
    "        linear.raw_features += [t[0] if isinstance(t[0], str) else t[0][0] for t in transformer[1].features]\n",
    "        \n",
    "    linear.space = {\n",
    "        'estimate__fit_intercept': hp.choice('estimate__fit_intercept', ['True', 'False']),\n",
    "    }\n",
    "\n",
    "    if hyperopt:\n",
    "        linear.run(do_lowess=DO_LOWESS)\n",
    "    else:\n",
    "        # train with default params\n",
    "        linear.pipeline.fit(X=linear.X_train, y=linear.y_train)\n",
    "        linear.model = linear.pipeline\n",
    "        linear.stats()\n",
    "        linear.plot_feature_importance()\n",
    "        linear.plot_predicted_vs_actual(do_lowess=DO_LOWESS)\n",
    "        linear.qq_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    #some parameters\n",
    "    DO_LOWESS = False\n",
    "    hyperopt = False\n",
    "    # Run Lasso Regression with hyperopt optimization\n",
    "    lasso = HyperoptModel(train.copy(), test.copy(),'lasso', cv=4, max_evals = 500)\n",
    "    lasso.raw_features = []\n",
    "    lasso.pipeline = Pipeline([\n",
    "          ('prepare_features', FeatureUnion([\n",
    "            ('user_features', user_features),\n",
    "            ('tweet_features', tweet_features),\n",
    "            ('retweet_features', retweet_features),\n",
    "            ('reply_features', reply_features)\n",
    "        ])),\n",
    "        ('estimate', linear_model.Lasso(**{'alpha': 0.0006577184991258585}))\n",
    "    ])\n",
    "    \n",
    "    for transformer in lasso.pipeline.named_steps['prepare_features'].transformer_list:\n",
    "        lasso.raw_features += [t[0] if isinstance(t[0], str) else t[0][0] for t in transformer[1].features]\n",
    "        \n",
    "    lasso.space = {\n",
    "        'estimate__alpha': hp.uniform('estimate__alpha', 0, 10),\n",
    "    }\n",
    "\n",
    "    if hyperopt:\n",
    "        lasso.run(do_lowess=DO_LOWESS)\n",
    "    else:\n",
    "        # train with default params\n",
    "        lasso.pipeline.fit(X=lasso.X_train, y=lasso.y_train)\n",
    "        lasso.model = lasso.pipeline\n",
    "        lasso.stats()\n",
    "        lasso.plot_feature_importance()\n",
    "        lasso.plot_predicted_vs_actual(do_lowess=DO_LOWESS)\n",
    "        lasso.qq_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #some parameters\n",
    "    DO_LOWESS = False\n",
    "    hyperopt = True\n",
    "    # Run Ridge Regression with hyperopt optimization\n",
    "    ridge = HyperoptModel(train.copy(), test.copy(),'ridge', cv=4, max_evals = 500)\n",
    "    ridge.raw_features = []\n",
    "\n",
    "    ridge.pipeline = Pipeline([\n",
    "         ('prepare_features', FeatureUnion([\n",
    "            ('user_features', user_features),\n",
    "            ('tweet_features', tweet_features),\n",
    "            ('retweet_features', retweet_features),\n",
    "            ('reply_features', reply_features)\n",
    "        ])),\n",
    "        ('estimate', linear_model.Ridge(**{'alpha': 0.05231780585024858}))\n",
    "    ])\n",
    "    \n",
    "    for transformer in ridge.pipeline.named_steps['prepare_features'].transformer_list:\n",
    "        ridge.raw_features += [t[0] if isinstance(t[0], str) else t[0][0] for t in transformer[1].features]\n",
    "        \n",
    "    ridge.space = {\n",
    "        'estimate__alpha': hp.uniform('estimate__alpha', 0, 1000),\n",
    "    }\n",
    "\n",
    "    if hyperopt:\n",
    "        ridge.run(do_lowess=DO_LOWESS)\n",
    "    else:\n",
    "        # train with default params\n",
    "        ridge.pipeline.fit(X=ridge.X_train, y=ridge.y_train)\n",
    "        ridge.model = ridge.pipeline\n",
    "        ridge.stats()\n",
    "        ridge.plot_feature_importance()\n",
    "        ridge.plot_predicted_vs_actual(do_lowess=DO_LOWESS)\n",
    "        ridge.qq_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #some parameters\n",
    "    DO_LOWESS = False\n",
    "    hyperopt = True\n",
    "    # Run MLPRegressor with hyperopt optimization\n",
    "    nn = HyperoptModel(train.copy(), test.copy(),'nn', cv=3, max_evals=500)\n",
    "    nn.raw_features = []\n",
    "\n",
    "    nn.pipeline = Pipeline([\n",
    "          ('prepare_features', FeatureUnion([\n",
    "            ('user_features', user_features),\n",
    "            ('tweet_features', tweet_features),\n",
    "            ('retweet_features', retweet_features),\n",
    "            ('reply_features', reply_features)\n",
    "        ])),\n",
    "        ('estimate', MLPRegressor())\n",
    "    ])\n",
    "    \n",
    "    for transformer in nn.pipeline.named_steps['prepare_features'].transformer_list:\n",
    "        nn.raw_features += [t[0] if isinstance(t[0], str) else t[0][0] for t in transformer[1].features]\n",
    "        \n",
    "    nn.space = {\n",
    "         'estimate__alpha' : hp.uniform('estimate__alpha', 0.001, 1),\n",
    "         'estimate__activation' : hp.choice('estimate__activation', ['logistic']), # 'identity', 'logistic', 'tanh', 'relu'\n",
    "         #'estimate__learning_rate' : hp.choice('estimate__learning_rate', ['constant', 'invscaling', 'adaptive']),\n",
    "         'estimate__hidden_layer_sizes' : scope.int(hp.uniform('estimate__hidden_layer_sizes', 1, 100)),\n",
    "         'estimate__solver' : hp.choice('estimate__solver', ['adam']), #'lbfgs', 'sgd',\n",
    "         #'estimate__max_iter' : scope.int(hp.uniform('estimate__max_iter', 500, 1000))\n",
    "    }\n",
    "\n",
    "    if hyperopt:\n",
    "        nn.run(do_lowess=DO_LOWESS)\n",
    "    else:\n",
    "        # train with default params\n",
    "        nn.pipeline.fit(X=nn.X_train, y=nn.y_train)\n",
    "        nn.model = cat.pipeline\n",
    "        nn.stats()\n",
    "        nn.plot_predicted_vs_actual(do_lowess=DO_LOWESS)\n",
    "        nn.plot_feature_importance()\n",
    "        nn.qq_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats (train | test):\n",
      "\tR^2 score:\t\t0.5490\n",
      "\t\t\t\t\t0.3172\n",
      "\tRMSE:\t\t\t0.0133\n",
      "\t\t\t\t\t0.0139\n",
      "\tMean error:\t\t0.0038\n",
      "\t\t\t\t\t0.0042\n",
      "\tPearson:\t\t0.7566\n",
      "\t\t\t\t\t0.5726\n",
      "\tSpearman:\t\t0.6162\n",
      "\t\t\t\t\t0.5716\n",
      "\tKendallTau:\t\t0.4665\n",
      "\t\t\t\t\t0.4339\n",
      "\n",
      "Plotting predicted vs. actual ...done\n",
      "\n",
      "[12.988011312458314, 22.699932324286472, 0.9508206146619843, 1.0363967959142244, 0.4416664732108111, 29.378013370058024, 8.56213259896625, 4.233722513273209, 3.8215911298931275, 6.959632954154819, 5.506524592517641, 1.380596137564508, 1.2420108904737448, 0.7989482925668469]\n",
      "Plotting feature importances ...done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    #some parameters\n",
    "    DO_LOWESS = False\n",
    "    hyperopt = False\n",
    "    # Run CatBoostRegressor with hyperopt optimization\n",
    "    cat = HyperoptModel(train.copy(), test.copy(),'cat', cv=3, max_evals = 30)\n",
    "    cat.raw_features = []\n",
    "    cat.pipeline = Pipeline([\n",
    "          ('prepare_features', FeatureUnion([\n",
    "            ('user_features', user_features),\n",
    "            ('tweet_features', tweet_features),\n",
    "            ('retweet_features', retweet_features),\n",
    "            ('reply_features', reply_features)\n",
    "        ])),\n",
    "        ('estimate', CatBoostRegressor())\n",
    "    ])\n",
    "     \n",
    "    for transformer in cat.pipeline.named_steps['prepare_features'].transformer_list:\n",
    "        cat.raw_features += [t[0] if isinstance(t[0], str) else t[0][0] for t in transformer[1].features]\n",
    "        \n",
    "    cat.space = {\n",
    "        'estimate__iterations': hp.choice('estimate__iterations', [1300]),\n",
    "        'estimate__loss_function': hp.choice('estimate__loss_function', ['RMSE']),\n",
    "        'estimate__train_dir': hp.choice('estimate__train_dir', ['outputs/cat']),\n",
    "        'estimate__thread_count': hp.choice('estimate__thread_count', [4]),\n",
    "        'estimate__used_ram_limit': hp.choice('estimate__used_ram_limit', [1024 * 1024 * 1024 * 4]),  # 4gb\n",
    "        'estimate__random_seed': hp.choice('estimate__random_seed', [0]),\n",
    "\n",
    "        'estimate__learning_rate': hp.loguniform('estimate__learning_rate', -5, 0),\n",
    "        'estimate__random_strength': hp.choice('estimate__random_strength', [1, 20]),\n",
    "        'estimate__l2_leaf_reg': hp.loguniform('estimate__l2_leaf_reg', 0, np.log(10)),\n",
    "        'estimate__bagging_temperature': hp.uniform('estimate__bagging_temperature', 0, 1),\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    find the best number of trees (following https://tech.yandex.com/catboost/doc/dg/concepts/parameter-tuning_trees-number-docpage/#parameter-tuning_trees-number)\n",
    "    need to also substitute the CatBoostRegressor parameters with:\n",
    "        ('estimate', CatBoostRegressor(iterations=10000, loss_function='RMSE', auto_stop_pval=1e-4, use_best_model=True, train_dir='outputs/cat_trees', verbose=True))\n",
    "    \"\"\"\n",
    "    # num_trees_train, num_trees_eval = train_test_split(train, test_size=0.2, random_state=0)\n",
    "    # X = num_trees_train[list(filter(lambda column: column in cat.raw_features, cat.train.columns))]\n",
    "    # y = num_trees_train['score']\n",
    "    #\n",
    "    # eval_X = num_trees_eval[list(filter(lambda column: column in cat.raw_features, cat.train.columns))]\n",
    "    # eval_X = cat.pipeline.named_steps['prepare_features'].fit_transform(eval_X)\n",
    "    # eval_set = (eval_X, num_trees_eval['score'])\n",
    "    #\n",
    "    # cat.pipeline.fit(X=X, y=y, estimate__eval_set=eval_set)\n",
    "    # cat.model = cat.pipeline\n",
    "    # print(cat.model.named_steps['estimate'].get_params())\n",
    "    # cat.model.named_steps['estimate'].save_model('tmp/cat.model'.encode('utf-8'))\n",
    "\n",
    "    if hyperopt:\n",
    "        cat.run(do_lowess=DO_LOWESS)\n",
    "    else:\n",
    "        # train with default params\n",
    "        cat.pipeline.fit(X=cat.X_train, y=cat.y_train)\n",
    "        cat.model = cat.pipeline\n",
    "        cat.stats()\n",
    "        cat.plot_predicted_vs_actual(do_lowess=DO_LOWESS)\n",
    "        cat.plot_feature_importance()\n",
    "        #cat.qq_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #some parameters\n",
    "    DO_LOWESS = False\n",
    "    hyperopt = True\n",
    "    # Run LGBMRegressor with hyperopt optimization\n",
    "    lgbm = HyperoptModel(train.copy(), test.copy(), 'lgbm', cv=3, max_evals=50)\n",
    "    lgbm.raw_features = []\n",
    "    lgbm.pipeline = Pipeline([\n",
    "          ('prepare_features', FeatureUnion([\n",
    "            ('user_features', user_features),\n",
    "            ('tweet_features', tweet_features),\n",
    "            ('retweet_features', retweet_features),\n",
    "            ('reply_features', reply_features)\n",
    "        ])),\n",
    "        ('estimate', lgb.LGBMRegressor())\n",
    "    ])\n",
    "    \n",
    "\n",
    "    \n",
    "    for transformer in lgbm.pipeline.named_steps['prepare_features'].transformer_list:\n",
    "        lgbm.raw_features += [t[0] if isinstance(t[0], str) else t[0][0] for t in transformer[1].features]\n",
    "    \"\"\" find number of trees \"\"\"\n",
    "    # num_trees_train, num_trees_eval = train_test_split(train, test_size=0.2, random_state=0)\n",
    "    # X = num_trees_train[list(filter(lambda column: column in lgbm.raw_features, lgbm.train.columns))]\n",
    "    # y = num_trees_train['score']\n",
    "    #\n",
    "    # eval_X = num_trees_eval[list(filter(lambda column: column in lgbm.raw_features, lgbm.train.columns))]\n",
    "    # eval_X = lgbm.pipeline.named_steps['prepare_features'].fit_transform(eval_X)\n",
    "    # eval_set = (eval_X, num_trees_eval['score'])\n",
    "    #\n",
    "    # best = lgbm.pipeline.fit(X=X, y=y, estimate__eval_set=eval_set, estimate__early_stopping_rounds=10)\n",
    "    # print(best.named_steps['estimate'].best_iteration)\n",
    "\n",
    "    lgbm.space = {\n",
    "        'estimate__objective': hp.choice('estimate__objective', ['regression']),\n",
    "        'estimate__n_estimators': hp.choice('estimate__n_estimators', [400]),\n",
    "        'estimate__seed': hp.choice('estimate__seed', [0]),\n",
    "\n",
    "        'estimate__learning_rate': hp.loguniform('estimate__learning_rate', -7, 0),\n",
    "        'estimate__num_leaves': scope.int(hp.qloguniform('estimate__num_leaves', 1, 7, 1)),\n",
    "        'estimate__feature_fraction': hp.uniform('estimate__feature_fraction', 0.5, 1),\n",
    "        'estimate__bagging_fraction': hp.uniform('estimate__bagging_fraction', 0.5, 1),\n",
    "        'estimate__min_data_in_leaf': scope.int(hp.qloguniform('estimate__min_data_in_leaf', 0, 6, 1)),\n",
    "        'estimate__min_sum_hessian_in_leaf': hp.loguniform('estimate__min_sum_hessian_in_leaf', -16, 5),\n",
    "        'estimate__lambda_l1': hp.choice('lambda_l1', [0, hp.loguniform('estimate__lambda_l1_positive', -16, 2)]),\n",
    "        'estimate__lambda_l2': hp.choice('lambda_l2', [0, hp.loguniform('estimate__lambda_l2_positive', -16, 2)]),\n",
    "    }\n",
    "\n",
    "    if hyperopt:\n",
    "        lgbm.run(do_lowess=DO_LOWESS)\n",
    "    else:\n",
    "        # train with default params\n",
    "        lgbm.pipeline.fit(X=lgbm.X_train, y=lgbm.y_train)\n",
    "        lgbm.model = lgbm.pipeline\n",
    "        lgbm.stats()\n",
    "        lgbm.plot_predicted_vs_actual()\n",
    "        lgbm.plot_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Polynomial linear regression \n",
    "    degrees = 4\n",
    "    for d in range(1, degrees+1):\n",
    "            print(\"Degree: %s\" % d)\n",
    "            # Create the model, split the sets and fit it\n",
    "            polynomial_features = PolynomialFeatures(\n",
    "                degree=d, include_bias=False\n",
    "            )\n",
    "            polynomial = HyperoptModel(train.copy(), test.copy(),'poly'+str(d), cv=4, max_evals = 1)\n",
    "            polynomial.raw_features = []\n",
    "            polynomial.pipeline = Pipeline([\n",
    "          ('prepare_features', FeatureUnion([\n",
    "            ('user_features', user_features),\n",
    "            ('tweet_features', tweet_features),\n",
    "            ('retweet_features', retweet_features),\n",
    "            ('reply_features', reply_features)\n",
    "        ])),\n",
    "         (\"polynomial_features\", polynomial_features),\n",
    "        ('estimate', linear_model.LinearRegression())\n",
    "    ])\n",
    "    \n",
    "            for transformer in polynomial.pipeline.named_steps['prepare_features'].transformer_list:\n",
    "                polynomial.raw_features += [t[0] if isinstance(t[0], str) else t[0][0] for t in transformer[1].features]\n",
    "            polynomial.space = {\n",
    "        'estimate__fit_intercept': hp.choice('estimate__fit_intercept', ['True', 'False']),\n",
    "    }\n",
    "\n",
    "            if hyperopt:\n",
    "                polynomial.run(do_lowess=DO_LOWESS)\n",
    "            else:\n",
    "                # train with default params\n",
    "                polynomial.pipeline.fit(X=polynomial.X_train, y=polynomial.y_train)\n",
    "                polynomial.model = polynomial.pipeline\n",
    "                polynomial.stats()\n",
    "                polynomial.plot_feature_importance()\n",
    "                polynomial.plot_predicted_vs_actual(do_lowess=DO_LOWESS)\n",
    "                polynomial.qq_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
